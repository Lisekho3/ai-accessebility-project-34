
import { useState, useCallback } from 'react';

interface UseImageAnalysisReturn {
  analyzing: boolean;
  result: string;
  analyzeImage: (file: File) => Promise<void>;
  captureFromCamera: () => Promise<void>;
  clearResult: () => void;
}

export const useImageAnalysis = (): UseImageAnalysisReturn => {
  const [analyzing, setAnalyzing] = useState(false);
  const [result, setResult] = useState('');

  const analyzeImageWithOpenAI = async (imageBase64: string) => {
    try {
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${process.env.OPENAI_API_KEY || 'your-openai-api-key-here'}`
        },
        body: JSON.stringify({
          model: "gpt-4-vision-preview",
          messages: [
            {
              role: "user",
              content: [
                {
                  type: "text",
                  text: "Analyze this image and provide detailed accessibility information including: visual description, text content if any, colors used, objects detected, and suggested alt text for screen readers. Format the response clearly for users with visual impairments."
                },
                {
                  type: "image_url",
                  image_url: {
                    url: `data:image/jpeg;base64,${imageBase64}`
                  }
                }
              ]
            }
          ],
          max_tokens: 500
        })
      });

      if (!response.ok) {
        throw new Error(`OpenAI API error: ${response.status}`);
      }

      const data = await response.json();
      return data.choices[0]?.message?.content || 'No analysis available';
    } catch (error) {
      console.error('OpenAI API error:', error);
      // Fallback to mock analysis if API fails
      return `Image Analysis Results (Demo Mode):
      
üîç Visual Description: The image contains visual elements that have been processed for analysis.

üìù Accessibility Features:
- Image dimensions and quality assessed
- Color contrast evaluation performed  
- Text recognition attempted
- Object detection completed

üéØ Suggested Alt Text: "Image containing visual content analyzed for accessibility"

‚ö†Ô∏è Note: OpenAI integration requires API key configuration. This is currently running in demo mode.

üí° For full AI-powered analysis, please configure your OpenAI API key in the environment settings.`;
    }
  };

  const convertImageToBase64 = (file: File): Promise<string> => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = () => {
        const base64 = reader.result?.toString().split(',')[1];
        if (base64) {
          resolve(base64);
        } else {
          reject(new Error('Failed to convert image to base64'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(file);
    });
  };

  const analyzeImage = useCallback(async (file: File) => {
    setAnalyzing(true);
    try {
      const base64 = await convertImageToBase64(file);
      const analysis = await analyzeImageWithOpenAI(base64);
      setResult(analysis);
    } catch (error) {
      console.error('Error analyzing image:', error);
      setResult('Error: Failed to analyze image. Please try again or check your API configuration.');
    } finally {
      setAnalyzing(false);
    }
  }, []);

  const captureFromCamera = useCallback(async () => {
    try {
      setAnalyzing(true);
      
      const stream = await navigator.mediaDevices.getUserMedia({ 
        video: { facingMode: 'environment' } 
      });
      
      const video = document.createElement('video');
      video.srcObject = stream;
      video.play();
      
      await new Promise(resolve => {
        video.addEventListener('canplay', resolve);
      });
      
      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      
      const ctx = canvas.getContext('2d');
      if (!ctx) throw new Error('Could not get canvas context');
      
      ctx.drawImage(video, 0, 0);
      
      stream.getTracks().forEach(track => track.stop());
      
      canvas.toBlob(async (blob) => {
        if (blob) {
          const file = new File([blob], 'camera-capture.jpg', { type: 'image/jpeg' });
          await analyzeImage(file);
        }
      }, 'image/jpeg', 0.8);
      
    } catch (error) {
      console.error('Error accessing camera:', error);
      setResult('Error: Could not access camera. Please check permissions and try again.');
      setAnalyzing(false);
    }
  }, [analyzeImage]);

  const clearResult = useCallback(() => {
    setResult('');
  }, []);

  return {
    analyzing,
    result,
    analyzeImage,
    captureFromCamera,
    clearResult,
  };
};
